{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class2label = {'Other': 0,\n",
    "               'Message-Topic(e1,e2)': 1, 'Message-Topic(e2,e1)': 2,\n",
    "               'Product-Producer(e1,e2)': 3, 'Product-Producer(e2,e1)': 4,\n",
    "               'Instrument-Agency(e1,e2)': 5, 'Instrument-Agency(e2,e1)': 6,\n",
    "               'Entity-Destination(e1,e2)': 7, 'Entity-Destination(e2,e1)': 8,\n",
    "               'Cause-Effect(e1,e2)': 9, 'Cause-Effect(e2,e1)': 10,\n",
    "               'Component-Whole(e1,e2)': 11, 'Component-Whole(e2,e1)': 12,\n",
    "               'Entity-Origin(e1,e2)': 13, 'Entity-Origin(e2,e1)': 14,\n",
    "               'Member-Collection(e1,e2)': 15, 'Member-Collection(e2,e1)': 16,\n",
    "               'Content-Container(e1,e2)': 17, 'Content-Container(e2,e1)': 18}\n",
    "\n",
    "label2class = {0: 'Other',\n",
    "               1: 'Message-Topic(e1,e2)', 2: 'Message-Topic(e2,e1)',\n",
    "               3: 'Product-Producer(e1,e2)', 4: 'Product-Producer(e2,e1)',\n",
    "               5: 'Instrument-Agency(e1,e2)', 6: 'Instrument-Agency(e2,e1)',\n",
    "               7: 'Entity-Destination(e1,e2)', 8: 'Entity-Destination(e2,e1)',\n",
    "               9: 'Cause-Effect(e1,e2)', 10: 'Cause-Effect(e2,e1)',\n",
    "               11: 'Component-Whole(e1,e2)', 12: 'Component-Whole(e2,e1)',\n",
    "               13: 'Entity-Origin(e1,e2)', 14: 'Entity-Origin(e2,e1)',\n",
    "               15: 'Member-Collection(e1,e2)', 16: 'Member-Collection(e2,e1)',\n",
    "               17: 'Content-Container(e1,e2)', 18: 'Content-Container(e2,e1)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from nltk.stem import SnowballStemmer\n",
    "Stemmer = SnowballStemmer('english')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "add_w={'to':0,'of':1,'by':2,'in':3,'inside':4,\n",
    "       'with':5,'from':6,'into':7,'on':8,'about':9}\n",
    "kk=add_w.keys()\n",
    "\n",
    "def relation_shorten(short_text):\n",
    "    tokens = nltk.word_tokenize(short_text)\n",
    "    pos_tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "    # 1.简化并列词\n",
    "    j = len(pos_tagged_tokens) - 2\n",
    "    while j > 0:\n",
    "        if pos_tagged_tokens[j][1] == 'CC':\n",
    "            del pos_tagged_tokens[j]\n",
    "            del tokens[j]\n",
    "            if j < len(pos_tagged_tokens) - 1:\n",
    "                del pos_tagged_tokens[j]\n",
    "                del tokens[j]\n",
    "        j -= 1\n",
    "\n",
    "    # 2.保留动词，名词，IN类型，to类型，人称代词和占有代词\n",
    "    rvbin = []\n",
    "    rpos = []\n",
    "    rvbin.append(tokens[0])\n",
    "    rpos.append(pos_tagged_tokens[0])\n",
    "    for k in range(1, len(tokens) - 1):\n",
    "        if pos_tagged_tokens[k][1] == 'TO' or 'VB' in pos_tagged_tokens[k][1] or pos_tagged_tokens[k][\n",
    "            1] == 'IN' or 'NN' in pos_tagged_tokens[k][1] or 'PRP' in pos_tagged_tokens[k][1]:\n",
    "            rvbin.append(tokens[k])\n",
    "            rpos.append(pos_tagged_tokens[k])\n",
    "    rvbin.append(tokens[-1])\n",
    "    rpos.append(pos_tagged_tokens[-1])\n",
    "\n",
    "    # 3.简化名词\n",
    "    j = len(rvbin) - 2\n",
    "    while j > 0:\n",
    "        if 'NN' in rpos[j][1]:\n",
    "            if rpos[j + 1][1] == 'NN' or rpos[j + 1][1] == 'NNS':\n",
    "                del rpos[j]\n",
    "                del rvbin[j]\n",
    "        j -= 1\n",
    "\n",
    "    if len(rvbin) > 2 and 'NN' in rpos[1][1]:\n",
    "        del rpos[1]\n",
    "        del rvbin[1]\n",
    "\n",
    "    # 4.简化两个实体的描述\n",
    "    j = len(rvbin) - 3\n",
    "    while j > 1:\n",
    "        if rpos[j][1] == 'NN' or rpos[j][1] == 'NNS':\n",
    "            if rpos[j + 1][0] == 'of' or rpos[j + 1][0] == 'for':\n",
    "                d = j\n",
    "                while d < len(rpos) - 1:\n",
    "                    del rpos[d]\n",
    "                    del rvbin[d]\n",
    "            elif rpos[j - 1][0] == 'of' or rpos[j - 1][0] == 'for':\n",
    "                d = j\n",
    "                while d > 0 and d < len(rpos) - 1:\n",
    "                    del rpos[d]\n",
    "                    del rvbin[d]\n",
    "                    d -= 1\n",
    "                    j -= 1\n",
    "        j -= 1\n",
    "\n",
    "    # 5.保留靠近e2的动作\n",
    "    j = len(rvbin) - 2\n",
    "    while j > 0:\n",
    "        if rpos[j][1] == 'NN' or rpos[j][1] == 'NNS':\n",
    "            d = j\n",
    "            while d > 0:\n",
    "                del rpos[d]\n",
    "                del rvbin[d]\n",
    "                d -= 1\n",
    "                j -= 1\n",
    "        j -= 1\n",
    "    # trigger word sequence\n",
    "    r = \" \".join(rvbin)\n",
    "\n",
    "    add_k=[]\n",
    "    for m in rvbin:\n",
    "        if m in kk:\n",
    "            add_k.append(add_w[m])\n",
    "    if len(add_k) == 0:\n",
    "        add_k.append(10)\n",
    "    add_rw_cate = keras.utils.to_categorical(add_k[-1], 11)\n",
    "\n",
    "    p = []\n",
    "    for w in range(len(rpos)):\n",
    "        if w == 0 or w == len(rpos) - 1:\n",
    "            if 'NN' not in rpos[w][1]:\n",
    "                p.append('NN')\n",
    "            else:\n",
    "                p.append(rpos[w][1])\n",
    "        else:\n",
    "            p.append(rpos[w][1])\n",
    "    p = \" \".join(p)\n",
    "\n",
    "    return r, p, add_rw_cate\n",
    "\n",
    "def relation_words(between_e):\n",
    "    words=[]\n",
    "    for i in between_e[0]:\n",
    "        i=i.split()\n",
    "        for j in i:\n",
    "            w=Stemmer.stem(j)\n",
    "            words.append(w)\n",
    "    for i in between_e[1]:\n",
    "        i=i.split()\n",
    "        for j in i:\n",
    "            w=Stemmer.stem(j)\n",
    "            words.append(w)\n",
    "    words=pd.value_counts(words)\n",
    "    return words\n",
    "\n",
    "def relation_words_between_entity(between_e,words):\n",
    "    rwbe=[]\n",
    "    for i in between_e:\n",
    "        wordslist=i.split()\n",
    "        rwbe_i=[]\n",
    "        for j in wordslist:\n",
    "            num_the=0\n",
    "            w=Stemmer.stem(j)\n",
    "            if words[w]>30 and j!='-' and j!='a' and j!='an' and j!='an':\n",
    "                if j!='the':\n",
    "                    rwbe_i.append(j)\n",
    "                else:\n",
    "                    num_the+=1\n",
    "                if num_the>1:\n",
    "                    rwbe_i=[]\n",
    "        if len(rwbe_i)==0:\n",
    "            rwbe_i.append('NANA')\n",
    "        rwbe_i=rwbe_i[-4:]\n",
    "        rwbe_i=\" \".join(rwbe_i)\n",
    "        rwbe.append(rwbe_i)\n",
    "    return rwbe\n",
    "\n",
    "def clean_str(text):\n",
    "    text = text.lower()\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"that's\", \"that is \", text)\n",
    "    text = re.sub(r\"there's\", \"there is \", text)\n",
    "    text = re.sub(r\"it's\", \"it is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def load_data_and_labels(path):\n",
    "    data = []\n",
    "    lines = [line.strip() for line in open(path)]\n",
    "    max_sentence_length = 0\n",
    "    for idx in range(0, len(lines), 4):\n",
    "        id = lines[idx].split(\"\\t\")[0]\n",
    "        relation = lines[idx + 1]\n",
    "\n",
    "        sentence = lines[idx].split(\"\\t\")[1][1:-1]\n",
    "        sentence = sentence.replace('<e1>', ' _e11_ ')\n",
    "        sentence = sentence.replace('</e1>', ' _e12_ ')\n",
    "        sentence = sentence.replace('<e2>', ' _e21_ ')\n",
    "        sentence = sentence.replace('</e2>', ' _e22_ ')\n",
    "\n",
    "        sentence = clean_str(sentence)\n",
    "        print(sentence)\n",
    "        \n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        #print(tokens)\n",
    "        if max_sentence_length < len(tokens):\n",
    "            max_sentence_length = len(tokens)\n",
    "            \n",
    "        e11 = tokens.index(\"e11\")+1\n",
    "        e12 = tokens.index(\"e12\")+1\n",
    "        e21 = tokens.index(\"e21\")+1\n",
    "        e22 = tokens.index(\"e22\")+1\n",
    "        \n",
    "        e1 = tokens.index(\"e12\") - 1\n",
    "        e2 = tokens.index(\"e22\") - 1\n",
    "\n",
    "        sentence = \" \".join(tokens)\n",
    "\n",
    "        # 两个实体之间的词\n",
    "        between_e = tokens[tokens.index(\"e11\"):tokens.index(\"e22\")+1]\n",
    "        between_e = \" \".join(between_e)\n",
    "\n",
    "        between_e = between_e.replace('e11', '')\n",
    "        between_e = between_e.replace('e12', '')\n",
    "        between_e = between_e.replace('e21', '')\n",
    "        between_e = between_e.replace('e22', '')\n",
    "\n",
    "        relationword, relationwordpos, add_rw_cate = relation_shorten(between_e)\n",
    "\n",
    "        data.append([id, sentence, e11, e12, e21, e22, e1,e2, relation, relationword, relationwordpos, add_rw_cate])\n",
    "\n",
    "    print(path)\n",
    "    print(\"max sentence length = {}\\n\".format(max_sentence_length))\n",
    "\n",
    "    df = pd.DataFrame(data=data, columns=[\"id\", \"sentence\", \"e11\", \"e12\", \"e21\", \"e22\",'e1', 'e2', \"relation\", \"relationword\", \"relationwordpos\", \"rw_cate\"])\n",
    "\n",
    "    pos1, pos2 = get_relative_position(df, 90)\n",
    "\n",
    "    df['label'] = [class2label[r] for r in df['relation']]\n",
    "\n",
    "    # Text Data\n",
    "    x_text = df['sentence'].tolist()\n",
    "    r=df['relation'].tolist()\n",
    "    \n",
    "    e11=df['e11'].tolist()\n",
    "    e12=df['e12'].tolist()\n",
    "    e21=df['e21'].tolist()\n",
    "    e22=df['e22'].tolist()\n",
    "\n",
    "    relationword = df['relationword'].tolist()\n",
    "    relationwordpos = df['relationwordpos'].tolist()\n",
    "    rw_cate = df['rw_cate'].tolist()\n",
    "\n",
    "\n",
    "    e1 = df['e1'].tolist()\n",
    "    e2 = df['e2'].tolist()\n",
    "\n",
    "    # Label Data\n",
    "    y = df['label']\n",
    "    labels_flat = y.values.ravel()\n",
    "    labels_count = np.unique(labels_flat).shape[0]\n",
    "\n",
    "    # convert class labels from scalars to one-hot vectors\n",
    "    # 0  => [1 0 0 0 0 ... 0 0 0 0 0]\n",
    "    # 1  => [0 1 0 0 0 ... 0 0 0 0 0]\n",
    "    # ...\n",
    "    # 18 => [0 0 0 0 0 ... 0 0 0 0 1]\n",
    "    def dense_to_one_hot(labels_dense, num_classes):\n",
    "        num_labels = labels_dense.shape[0]\n",
    "        index_offset = np.arange(num_labels) * num_classes\n",
    "        labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "        return labels_one_hot\n",
    "\n",
    "    labels = dense_to_one_hot(labels_flat, labels_count)\n",
    "    labels = labels.astype(np.uint8)\n",
    "\n",
    "    return x_text, labels, e11, e12, e21, e22, pos1, pos2, relationword, relationwordpos, rw_cate,r\n",
    "\n",
    "\n",
    "def get_relative_position(df, max_sentence_length):\n",
    "    # Position data\n",
    "    pos1 = []\n",
    "    pos2 = []\n",
    "    for df_idx in range(len(df)):\n",
    "        sentence = df.iloc[df_idx]['sentence']\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        e1 = df.iloc[df_idx]['e1']\n",
    "        e2 = df.iloc[df_idx]['e2']\n",
    "\n",
    "        p1 = \"\"\n",
    "        p2 = \"\"\n",
    "        for word_idx in range(len(tokens)):\n",
    "            p1 += str((max_sentence_length - 1) + word_idx - e1) + \" \"\n",
    "            p2 += str((max_sentence_length - 1) + word_idx - e2) + \" \"\n",
    "        pos1.append(p1)\n",
    "        pos2.append(p2)\n",
    "\n",
    "    return pos1, pos2\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#/SemEval2010_task8_all_data/SemEval2010_task8_testing_keys/TEST_FILE_FULL.TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path='/SemEval2010_task8_all_data/SemEval2010_task8_training/TRAIN_FILE.TXT'\n",
    "train_text, train_y, e11,e12,e21,e22, train_pos1, train_pos2, train_rw, train_rw_pos, train_rw_cate, r = load_data_and_labels(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = [line.strip() for line in open(train_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "si=[]\n",
    "for i in range(len(train_rw)):\n",
    "    rr=train_rw[i].split()\n",
    "    rr.insert(1,\"#\")\n",
    "    rr.insert(-1,\"$\") \n",
    "    rr=\" \".join(rr)\n",
    "    si.append(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('mytrain.tsv','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f.write('\\n')\n",
    "for i in range(len(train_text)):\n",
    "    lenth=len(train_text[i].split())+1\n",
    "    f.write(str(r[i]+'\\t'+'1'+'\\t'+'2'+'\\t'+train_text[i]+'\\t'+si[i]))\n",
    "    f.write('\\t'+str(e11[i])+'\\t'+str(e12[i])+'\\t'+str(e21[i])+'\\t'+str(e22[i])+'\\t'+str(lenth))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
