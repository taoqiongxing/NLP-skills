{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, Reshape\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Pinkie Pie was here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/Ax/Desktop/tqx/关系抽取/Attention_Network_With_Keras-master/data/Time Dataset.json','r') as f:\n",
    "    dataset = json.loads(f.read())\n",
    "with open('C:/Users/Ax/Desktop/tqx/关系抽取/Attention_Network_With_Keras-master/data/Time Vocabs.json','r') as f:\n",
    "    human_vocab, machine_vocab = json.loads(f.read())\n",
    "    \n",
    "human_vocab_size = len(human_vocab)\n",
    "machine_vocab_size = len(machine_vocab)\n",
    "\n",
    "# Number of training examples\n",
    "m = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"\n",
    "    A method for tokenizing data.\n",
    "    \n",
    "    Inputs:\n",
    "    dataset - A list of sentence data pairs.\n",
    "    human_vocab - A dictionary of tokens (char) to id's.\n",
    "    machine_vocab - A dictionary of tokens (char) to id's.\n",
    "    Tx - X data size\n",
    "    Ty - Y data size\n",
    "    \n",
    "    Outputs:\n",
    "    X - Sparse tokens for X data\n",
    "    Y - Sparse tokens for Y data\n",
    "    Xoh - One hot tokens for X data\n",
    "    Yoh - One hot tokens for Y data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Metadata\n",
    "    m = len(dataset)\n",
    "    \n",
    "    # Initialize\n",
    "    X = np.zeros([m, Tx], dtype='int32')\n",
    "    Y = np.zeros([m, Ty], dtype='int32')\n",
    "    \n",
    "    # Process data\n",
    "    for i in range(m):\n",
    "        data = dataset[i]\n",
    "        X[i] = np.array(tokenize(data[0], human_vocab, Tx))\n",
    "        Y[i] = np.array(tokenize(data[1], machine_vocab, Ty))\n",
    "    \n",
    "    # Expand one hots\n",
    "    Xoh = oh_2d(X, len(human_vocab))\n",
    "    Yoh = oh_2d(Y, len(machine_vocab))\n",
    "    \n",
    "    return (X, Y, Xoh, Yoh)\n",
    "    \n",
    "def tokenize(sentence, vocab, length):\n",
    "    \"\"\"\n",
    "    Returns a series of id's for a given input token sequence.\n",
    "    \n",
    "    It is advised that the vocab supports <pad> and <unk>.\n",
    "    \n",
    "    Inputs:\n",
    "    sentence - Series of tokens\n",
    "    vocab - A dictionary from token to id\n",
    "    length - Max number of tokens to consider\n",
    "    \n",
    "    Outputs:\n",
    "    tokens - \n",
    "    \"\"\"\n",
    "    tokens = [0]*length\n",
    "    for i in range(length):\n",
    "        char = sentence[i] if i < len(sentence) else \"<pad>\"\n",
    "        char = char if (char in vocab) else \"<unk>\"\n",
    "        tokens[i] = vocab[char]\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "def ids_to_keys(sentence, vocab):\n",
    "    \"\"\"\n",
    "    Converts a series of id's into the keys of a dictionary.\n",
    "    \"\"\"\n",
    "    return [list(vocab.keys())[id] for id in sentence]\n",
    "\n",
    "def oh_2d(dense, max_value):\n",
    "    \"\"\"\n",
    "    Create a one hot array for the 2D input dense array.\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    oh = np.zeros(np.append(dense.shape, [max_value]))\n",
    "    \n",
    "    # Set correct indices\n",
    "    ids1, ids2 = np.meshgrid(np.arange(dense.shape[0]), np.arange(dense.shape[1]))\n",
    "    \n",
    "    oh[ids1.flatten(), ids2.flatten(), dense.flatten('F').astype(int)] = 1\n",
    "    \n",
    "    return oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 41 # Max x sequence length\n",
    "Ty = 5 # y sequence length\n",
    "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "# Split data 80-20 between training and test\n",
    "train_size = int(0.8*m)\n",
    "Xoh_train = Xoh[:train_size]\n",
    "Yoh_train = Yoh[:train_size]\n",
    "Xoh_test = Xoh[train_size:]\n",
    "Yoh_test = Yoh[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data point 4.\n",
      "\n",
      "The data input is: 8:25\n",
      "The data output is: 08:25\n",
      "\n",
      "The tokenized input is:[11 13  5  8 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40\n",
      " 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40 40]\n",
      "The tokenized output is: [ 0  8 10  2  5]\n",
      "\n",
      "The one-hot input is: [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "The one-hot output is: [[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "print(\"Input data point \" + str(i) + \".\")\n",
    "print(\"\")\n",
    "print(\"The data input is: \" + str(dataset[i][0]))\n",
    "print(\"The data output is: \" + str(dataset[i][1]))\n",
    "print(\"\")\n",
    "print(\"The tokenized input is:\" + str(X[i]))\n",
    "print(\"The tokenized output is: \" + str(Y[i]))\n",
    "print(\"\")\n",
    "print(\"The one-hot input is:\", Xoh[i])\n",
    "print(\"The one-hot output is:\", Yoh[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
